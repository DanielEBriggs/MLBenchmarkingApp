# Trains the discriminator
d_loss <- discriminator %>% train_on_batch(combined_images, labels)
d_loss[[2]]
d_loss[[2]]
labels
# Trains the discriminator
d_loss <- discriminator %>% train_on_batch(combined_images, labels)
d_loss
d_loss
# Trains the discriminator
d_loss <- discriminator %>% train_on_batch(combined_images, labels)
d_loss
d_loss
d_loss
# Trains the discriminator
d_loss <- discriminator %>% train_on_batch(combined_images, labels)
d_loss
discriminator %>% predict_proba(combined_images)
discriminator %>% predict(combined_images)
x_train <- as.matrix(data2)
D1 <- dim(x_train)
iterations <- 10000
batch_size <- 256
save_dir <- "data_gen"
dir.create(save_dir)
save <- as.data.frame(matrix(rep(NA, 1000*12*2),nrow = 2000))
place = 1
# Start the training loop
start <- 1
d_loss_vec <- rep(NA,iterations)
a_loss_vec <- rep(NA,iterations)
set.seed(1234)
for (step in 1:iterations) {
for(i in 1:3){
# Samples random points in the latent space
random_latent_vectors <- matrix(rnorm(batch_size * latent_dim),
nrow = batch_size, ncol = latent_dim)
# Decodes them to fake images
generated_images <- generator %>% predict(random_latent_vectors)
# Combines them with real images
stop <- start + batch_size - 1
real_images <- x_train[sample(1:nrow(x_train),batch_size),]
rows <- nrow(real_images)
combined_images <- matrix(rep(NA, batch_size*2 *1 * 12),nrow = batch_size*2)
combined_images[1:rows,] <- generated_images
combined_images[(rows+1):(rows*2),] <- real_images
combined_images <- array_reshape(combined_images, dim = c(nrow(combined_images),12,1))
# Assembles labels discriminating real from fake images
labels <- rbind(matrix(1, nrow = batch_size, ncol = 1),
matrix(0, nrow = batch_size, ncol = 1))
# Adds random noise to the labels -- an important trick!
labels <- labels + (0.25 * array(runif(prod(dim(labels))),
dim = dim(labels)))
# Trains the discriminator
d_loss <- discriminator %>% train_on_batch(combined_images, labels)
}
d_loss_vec[step] <- d_loss[[1]]
d_acc_vec <- d_loss[[2]]
# Samples random points in the latent space
random_latent_vectors <- matrix(rnorm(batch_size * latent_dim),
nrow = batch_size, ncol = latent_dim)
# Assembles labels that say "all real images"
misleading_targets <- array(0, dim = c(batch_size, 1)) + 0.25 * array(runif(batch_size),
dim = c(batch_size,1))
# Trains the generator (via the gan model, where the
# discriminator weights are frozen)
a_loss <- gan %>% train_on_batch(
random_latent_vectors,
misleading_targets
)
a_loss_vec[step] <- a_loss
start <- start + batch_size
if (start > (nrow(x_train) - batch_size))
start <- 1
# Occasionally saves images
if (step %% 100 == 0) {
# Saves model weights
print(step)
save_model_weights_hdf5(gan, "gan.h5")
# Prints metrics
cat("discriminator loss:", d_loss, "\n")
cat("adversarial loss:", a_loss, "\n")
save[place,] <- c(generated_images[1,,],0)
place <- place + 1
# Saves one real image for comparison
save[place,] <- c(real_images[1,],1)
place <- place + 1
}
}
met <- data.frame(adv = a_loss_vec[1:step], dis = d_loss_vec[1:step], step = 1:step)
met %>% ggplot() + geom_line(aes(x = step, y = adv),col = 'green',alpha = 0.5) +
geom_line(aes(x = step, y = dis), col = 'red', alpha = 0.5) + theme_classic() +
xlab("Iteration") + ylab("Loss")
random_latent_vectors <- matrix(rnorm(latent_dim),
nrow = 1, ncol = latent_dim)
# Decodes them to fake images
(generated_images <- generator %>% predict(random_latent_vectors))
(pred_prob <- discriminator %>% predict(combined_images))
step
1:iterations
generator_input <- layer_input(shape = c(latent_dim))
generator_output <- generator_input %>%
layer_dense(units = 512,kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_gaussian_dropout(0.2) %>%
layer_reshape()
layer_dense(units = 512,kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_gaussian_dropout(0.2)%>%
layer_dense(units = 512, kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_gaussian_dropout(0.2) %>%
layer_dense(units = 512, kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_dense(units = 512, kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_dense(units = 12, activation = 'tanh') %>%
layer_reshape(target_shape = c(12,1))
generator <- keras_model(generator_input,generator_output)
dim(generator %>% predict_on_batch(matrix(rnorm(500),nrow = 10)))
generator %>% compile(
optimizer = optimizer_adam(lr = 0.00001),
loss = "binary_crossentropy"
)
discriminator_input <- layer_input(shape = c(12, 1))
discriminator_output <- discriminator_input %>%
layer_dense(units = 256, ) %>%
layer_activation_leaky_relu() %>%
layer_gaussian_dropout(0.3) %>%
layer_dense(units = 256, activation = 'relu') %>%
layer_dense(units = 256, activation = 'relu') %>%
# Classification layer
layer_flatten() %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(units = 1, activation = "sigmoid")
discriminator <- keras_model(discriminator_input, discriminator_output)
summary(discriminator)
# To stabilize training, we use learning rate decay
# and gradient clipping (by value) in the optimizer.
discriminator_optimizer <- optimizer_adam(
lr = 0.005,
clipvalue = 0.5,
decay = 1e-6
)
freeze_weights(discriminator)
discriminator %>% compile(
optimizer = discriminator_optimizer,
loss = "binary_crossentropy",
metrics = 'accuracy'
)
gan_input <- layer_input(shape = c(latent_dim))
gan_output <- discriminator(generator(gan_input))
gan <- keras_model(gan_input, gan_output)
gan_optimizer <- optimizer_adam(
lr = 0.0005,
clipvalue = 0.5,
decay = 1e-6
)
gan %>% compile(
optimizer = gan_optimizer,
loss = "binary_crossentropy"
)
library(dplyr)
data <- read_csv('C:/Users/brigdani/Downloads/crime.csv/crime.csv')
head(data)
data2 <- mutate_if(data, is_character, as_factor)
data2 <- mutate_if(data2, is.factor, as.numeric)
data2 <- data2[complete.cases(data),]
head(data2)
x_train <- as.matrix(data2)
D1 <- dim(x_train)
iterations <- 10000
batch_size <- 256
save_dir <- "data_gen"
dir.create(save_dir)
save <- as.data.frame(matrix(rep(NA, 1000*12*2),nrow = 2000))
place = 1
# Start the training loop
start <- 1
d_loss_vec <- rep(NA,iterations)
a_loss_vec <- rep(NA,iterations)
set.seed(1234)
for (step in 1:iterations) {
for(i in 1:3){
# Samples random points in the latent space
random_latent_vectors <- matrix(rnorm(batch_size * latent_dim),
nrow = batch_size, ncol = latent_dim)
# Decodes them to fake images
generated_images <- generator %>% predict(random_latent_vectors)
# Combines them with real images
stop <- start + batch_size - 1
real_images <- x_train[sample(1:nrow(x_train),batch_size),]
rows <- nrow(real_images)
combined_images <- matrix(rep(NA, batch_size*2 *1 * 12),nrow = batch_size*2)
combined_images[1:rows,] <- generated_images
combined_images[(rows+1):(rows*2),] <- real_images
combined_images <- array_reshape(combined_images, dim = c(nrow(combined_images),12,1))
# Assembles labels discriminating real from fake images
labels <- rbind(matrix(1, nrow = batch_size, ncol = 1),
matrix(0, nrow = batch_size, ncol = 1))
# Adds random noise to the labels -- an important trick!
labels <- labels + (0.25 * array(runif(prod(dim(labels))),
dim = dim(labels)))
# Trains the discriminator
d_loss <- discriminator %>% train_on_batch(combined_images, labels)
}
d_loss_vec[step] <- d_loss[[1]]
d_acc_vec <- d_loss[[2]]
# Samples random points in the latent space
random_latent_vectors <- matrix(rnorm(batch_size * latent_dim),
nrow = batch_size, ncol = latent_dim)
# Assembles labels that say "all real images"
misleading_targets <- array(0, dim = c(batch_size, 1)) + 0.25 * array(runif(batch_size),
dim = c(batch_size,1))
# Trains the generator (via the gan model, where the
# discriminator weights are frozen)
a_loss <- gan %>% train_on_batch(
random_latent_vectors,
misleading_targets
)
a_loss_vec[step] <- a_loss
start <- start + batch_size
if (start > (nrow(x_train) - batch_size))
start <- 1
# Occasionally saves images
if (step %% 100 == 0) {
# Saves model weights
print(step)
save_model_weights_hdf5(gan, "gan.h5")
# Prints metrics
cat("discriminator loss:", d_loss[[1]], "\n")
cat("adversarial loss:", a_loss, "\n")
save[place,] <- c(generated_images[1,,],0)
place <- place + 1
# Saves one real image for comparison
save[place,] <- c(real_images[1,],1)
place <- place + 1
}
}
source('C:/Users/brigdani/Desktop/FunWithGans.R', echo=TRUE)
warnings()
data
cov(data)
cov(x_train)
cor(x_train)
corrplot::corrplot(cor(x_train))
corrplot::corrplot(cov(x_train))
corrplot::corrplot(cov(x_train)/max(cov(x_train)))
corrplot::corrplot(abs(cov(x_train))/max(cov(x_train)))
library(keras)
library(tensorflow)
library(tidyverse)
use_condaenv('r-tensorflow')
use_backend(backend = 'tensorflow')
latent_dim <- 50
height <- 1
width <- 12
generator_input <- layer_input(shape = c(latent_dim))
generator_output <- generator_input %>%
layer_dense(units = 512,kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_gaussian_dropout(0.2) %>%
layer_reshape()
generator_output <- generator_input %>%
layer_dense(units = 512,kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_gaussian_dropout(0.2) %>%
layer_dense(units = 512,kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_gaussian_dropout(0.2)%>%
layer_dense(units = 512, kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_gaussian_dropout(0.2) %>%
layer_dense(units = 512, kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_dense(units = 512, kernel_initializer = 'glorot_normal', activation = "relu") %>%
layer_batch_normalization() %>%
layer_dense(units = 12, activation = 'tanh') %>%
layer_reshape(target_shape = c(width,height))
generator <- keras_model(generator_input,generator_output)
dim(generator %>% predict_on_batch(matrix(rnorm(500),nrow = 10)))
print(generator %>% predict_on_batch(matrix(rnorm(500),nrow = 10)))
dim(generator %>% predict_on_batch(matrix(runif(500),nrow = 10)))
print(generator %>% predict_on_batch(matrix(runif(500),nrow = 10)))
generator %>% compile(
optimizer = optimizer_adam(lr = 0.00001),
loss = "binary_crossentropy"
)
c(width,height)
discriminator_input <- layer_input(shape = c(width,height))
discriminator_output <- discriminator_input %>%
layer_dense(units = 256, ) %>%
layer_activation_leaky_relu() %>%
layer_gaussian_dropout(0.3) %>%
layer_dense(units = 256, activation = 'relu') %>%
layer_dense(units = 256, activation = 'relu') %>%
# Classification layer
layer_flatten() %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(units = 1, activation = "sigmoid")
discriminator_output <- discriminator_input %>%
layer_dense(units = 256, activation = 'relu') %>%
layer_activation_leaky_relu() %>%
layer_gaussian_dropout(0.3) %>%
layer_dense(units = 256, activation = 'relu') %>%
layer_dense(units = 256, activation = 'relu') %>%
# Classification layer
layer_flatten() %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(units = 1, activation = "sigmoid")
discriminator <- keras_model(discriminator_input, discriminator_output)
summary(discriminator)
# To stabilize training, we use learning rate decay
# and gradient clipping (by value) in the optimizer.
discriminator_optimizer <- optimizer_adam(
lr = 0.005,
clipvalue = 0.5,
decay = 1e-6
)
# To stabilize training, we use learning rate decay
# and gradient clipping (by value) in the optimizer.
discriminator_optimizer <- optimizer_adam(
lr = 0.001,
clipvalue = 0.5,
decay = 1e-6
)
freeze_weights(discriminator)
discriminator %>% compile(
optimizer = discriminator_optimizer,
loss = "binary_crossentropy",
metrics = 'accuracy'
)
gan_input <- layer_input(shape = c(latent_dim))
gan_output <- discriminator(generator(gan_input))
gan <- keras_model(gan_input, gan_output)
gan_optimizer <- optimizer_adam(
lr = 0.0005,
clipvalue = 0.5,
decay = 1e-6
)
gan %>% compile(
optimizer = gan_optimizer,
loss = "binary_crossentropy"
)
gan %>% compile(
optimizer = gan_optimizer,
loss = "rms_prop"
)
?compile()
gan %>% compile(
optimizer = gan_optimizer,
loss = 'binary_cross_entropy',
metrics = 'mean_squared_error'
)
gan %>% compile(
optimizer = gan_optimizer,
loss = 'binary_crossentropy',
metrics = 'mean_squared_error'
)
library(dplyr)
data <- read_csv('C:/Users/brigdani/Downloads/crime.csv/crime.csv')
head(data)
data2 <- mutate_if(data, is_character, as_factor)
data2 <- mutate_if(data2, is.factor, as.numeric)
data2 <- data2[complete.cases(data),]
head(data2)
normalize(head(data2))
head(data2)
normalize(as.matrix(head(data2)))
x_train <- normalize(as.matrix(data2))
data2
x_train
D1 <- dim(x_train)
D1
batch_size <- 200
save_dir <- "data_gen"
dir.create(save_dir)
save <- as.data.frame(matrix(rep(NA, 1000*13*2),nrow = 2000))
place = 1
start <- 1
d_loss_vec <- rep(NA,iterations)
a_loss_vec <- rep(NA,iterations)
d_loss_vec
set.seed(1234)
random_latent_vectors <- matrix(rnorm(batch_size * latent_dim),
nrow = batch_size, ncol = latent_dim)
# Decodes them to fake images
generated_images <- generator %>% predict(random_latent_vectors)
stop <- start + batch_size - 1
real_images <- x_train[sample(1:nrow(x_train),batch_size),]
rows <- nrow(real_images)
combined_images <- matrix(rep(NA, batch_size*2 *1 * 12),nrow = batch_size*2)
combined_images[1:rows,] <- generated_images
combined_images[(rows+1):(rows*2),] <- real_images
combined_images <- array_reshape(combined_images, dim = c(nrow(combined_images),12,1))
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
sample(1:nrow(x_train),batch_size)
real_images <- x_train[sample(1:nrow(x_train),batch_size),]
rows <- nrow(real_images)
combined_images <- matrix(rep(NA, batch_size*2 *1 * 12),nrow = batch_size*2)
combined_images[1:rows,] <- generated_images
combined_images[(rows+1):(rows*2),] <- real_images
combined_images <- array_reshape(combined_images, dim = c(nrow(combined_images),12,1))
1e4 == 1000
1e4 == 10000
tensorflow::install_tensorflow(method = 'gpu')
tensorflow::install_tensorflow(version = 'gpu')
tensorflow::install_tensorflow(version = 'gpu')
shiny::runApp('C:/Users/brigdani/Desktop/MLBApp')
install.packages("kernlab")
library(kernlab)
runApp('C:/Users/brigdani/Desktop/MLBApp')
'kernlab' %in% installed.packages()
source('C:/Users/brigdani/Desktop/MLBApp/init.R', echo=TRUE)
runApp('C:/Users/brigdani/Desktop/MLBApp')
Tappy <- read.delim("TappyDemo.tsv")
Tappy <- read.delim("TappyDemo.tsv")
getwd()
source('C:/Users/brigdani/Desktop/MLBApp/init.R', echo=TRUE)
runApp('C:/Users/brigdani/Desktop/MLBApp')
curl <- getCurlHandle()
install.packages("RCurl")
install.packages("RCurl")
curl <- RCurl::getCurlHandle()
curcl
curl
Sys.setenv(http_proxy="http://webproxy.merck.com:8080")
Sys.setenv(https_proxy="https://webproxy.merck.com:8080")
Sys.setenv(http_proxy="http://webproxy.merck.com:8080")
Sys.setenv(https_proxy="https://webproxy.merck.com:8080")
source('C:/Users/brigdani/Desktop/MLBApp/init.R', echo=TRUE)
getwd()
read.csv("../Desktop/MLBApp/SITA_DiffPrivData.csv")
SITA <- read.csv("../Desktop/MLBApp/SITA_DiffPrivData.csv")
View(SITA)
View(SITA)
library(shiny)
Sys.setenv(http_proxy="http://webproxy.merck.com:8080")
Sys.setenv(https_proxy="https://webproxy.merck.com:8080")
options(repos = c(CRAN = "https://cran.rstudio.com"))
runApp('C:/Users/brigdani/Desktop/MLBApp')
library(shiny)
runApp('C:/Users/brigdani/Desktop/MLBApp')
getwd()
runApp('C:/Users/brigdani/Desktop/MLBApp')
runApp('C:/Users/brigdani/Desktop/MLBApp')
setwd("C:/Users/brigdani/Desktop/MLBApp")
Tappy <- read.delim("TappyDemo.tsv")
Microbes <- read.delim("Microbes.tsv")
runApp()
runApp()
getwd()
runApp()
.Library
runApp()
runApp()
runApp()
runApp()
row_number(iris)
#take name of model, combine it with tuned hyper parameters to give an informative name
#ie rf-3 would be a name for a random forest with mtry = 3
#combine to form a single dataframe with model names and all results
apply(res,1,function(r) paste(c(round(as.numeric(r[1:(ncol(res)-6)]),digits = 7),row_number(res)),collapse = '-')) %>%
paste(name,.,sep='-') -> model
runApp()
runApp()
sample(1:1000,1)
runApp()
shiny::runApp()
.libPaths()
.libPaths()
shiny::runApp()
Sys.getenv()
length(getLoadedDLLs())
loadedDLLs()
getloadedDLLs()
getLoadedDLLs()
source('C:/Users/brigdani/Desktop/MLBApp/init.R', echo=TRUE)
length(getLoadedDLLs())
runApp()
length(getLoadedDLLs())
length(getLoadedDLLs())
R.home()
list.files(R.home())
list.files(paste(R.home(),"/etc",collapse = "")
)
list.files(paste(R.home(),"/etc",paste = "")
)
list.files(paste(R.home(),"/etc",paste = ""))
paste(R.home(),"/etc",paste = "")
paste(R.home(),"/etc",collapse = "")
paste(c(R.home(),"/etc"),collapse = "")
list.files(paste(c(R.home(),"/etc"),collapse = ""))
list.files(paste(c(R.home(),"/etc","/Rcmd_environ"),collapse = ""))
getLoadedDLLs()
length(getLoadedDLLs())
DLL.version()
pmax(100, pmin(1000, 0.6* OS_dependent_getrlimit_or_equivalent()))
dyn.load()
runApp()
length(getLoadedDLLs())
getLoadedDLLs()
